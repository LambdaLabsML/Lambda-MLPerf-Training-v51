## MLPerf Training v5.1 Unofficial Submission Results for Lambda
Lambda created an official submission for the latest round of MLPerf Training benchmarking, where we were able to submit results for the Llama2 70b LoRa and Llama3.1 8b models.
We were able to create acceptable results on a full-rack (18 nodes) for GB300s, amongst only two other submitters. However, our B200 submissions were rejected due to faulty
performance. In light of this, we have posthumously rerun these benchmarks such that we could achieve results that are on-par with industry leaders such as Oracle, Dell, and
NVIDIA. 
